{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Article Page Info MediaWiki API Example\n",
    "This example illustrates how to access page info data using the [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page). This example shows how to request summary 'page info' for a single article page. The API documentation, [API:Info](https://www.mediawiki.org/wiki/API:Info), covers additional details that may be helpful when trying to use or understand this example.\n",
    "\n",
    "## License\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.2 - September 16, 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example relies on some constants that help make the code a bit more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': ' swarali@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    if API_HEADER_AGENT not in headers:\n",
    "        raise Exception(f\"The header data should include a '{API_HEADER_AGENT}' field that contains your UW email address.\")\n",
    "\n",
    "    if 'uwnetid@uw' in headers[API_HEADER_AGENT]:\n",
    "        raise Exception(f\"Use your UW email address in the '{API_HEADER_AGENT}' field.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page info data for: Chinook salmon\n",
      "Invalid leading whitespace, reserved character(s), or return character(s) in header value: ' swarali@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "print(f\"Getting page info data for: {ARTICLE_TITLES[3]}\")\n",
    "info = request_pageinfo_per_article(ARTICLE_TITLES[3])\n",
    "print(json.dumps(info,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page info data for: Northern flicker\n",
      "{\n",
      "    \"351590\": {\n",
      "        \"pageid\": 351590,\n",
      "        \"ns\": 0,\n",
      "        \"title\": \"Northern flicker\",\n",
      "        \"contentmodel\": \"wikitext\",\n",
      "        \"pagelanguage\": \"en\",\n",
      "        \"pagelanguagehtmlcode\": \"en\",\n",
      "        \"pagelanguagedir\": \"ltr\",\n",
      "        \"touched\": \"2024-08-16T10:34:47Z\",\n",
      "        \"lastrevid\": 1237967172,\n",
      "        \"length\": 32225,\n",
      "        \"watchers\": 113,\n",
      "        \"talkid\": 8324488,\n",
      "        \"fullurl\": \"https://en.wikipedia.org/wiki/Northern_flicker\",\n",
      "        \"editurl\": \"https://en.wikipedia.org/w/index.php?title=Northern_flicker&action=edit\",\n",
      "        \"canonicalurl\": \"https://en.wikipedia.org/wiki/Northern_flicker\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Getting page info data for: {ARTICLE_TITLES[1]}\")\n",
    "info = request_pageinfo_per_article(ARTICLE_TITLES[1])\n",
    "print(json.dumps(info['query']['pages'],indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to get the information for multiple pages at the same time, by separating the page titles with the vertical bar \"|\" character. However, this approach has limits. You should probably check the API documentation if you want to do multiple pages in a single request - and limit the number of pages in one request reasonably.\n",
    "\n",
    "This example also illustrates creating a copy of the template, setting values in the template, and then calling the function using the template to supply the parameters for the API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page info data for: Bison|Red squirrel|Horseshoe bat\n",
      "Invalid leading whitespace, reserved character(s), or return character(s) in header value: ' swarali@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m request_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitles\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m page_titles\n\u001b[1;32m      5\u001b[0m info \u001b[38;5;241m=\u001b[39m request_pageinfo_per_article(request_template\u001b[38;5;241m=\u001b[39mrequest_info)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpages\u001b[39m\u001b[38;5;124m'\u001b[39m],indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "page_titles = f\"{ARTICLE_TITLES[0]}|{ARTICLE_TITLES[2]}|{ARTICLE_TITLES[4]}\"\n",
    "print(f\"Getting page info data for: {page_titles}\")\n",
    "request_info = PAGEINFO_PARAMS_TEMPLATE.copy()\n",
    "request_info['titles'] = page_titles\n",
    "info = request_pageinfo_per_article(request_template=request_info)\n",
    "print(json.dumps(info['query']['pages'],indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
